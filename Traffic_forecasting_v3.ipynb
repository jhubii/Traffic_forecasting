{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590f5076-f76b-4c64-b361-ad68362aa9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzier/null/work/Traffic-Forecasting/traffic_forecasting/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:774: UserWarning: Gradients do not exist for variables ['bidirectional_4/forward_lstm_8/lstm_cell/recurrent_kernel', 'bidirectional_4/forward_lstm_8/lstm_cell/bias', 'bidirectional_4/backward_lstm_8/lstm_cell/recurrent_kernel', 'bidirectional_4/backward_lstm_8/lstm_cell/bias', 'bidirectional_5/forward_lstm_9/lstm_cell/kernel', 'bidirectional_5/forward_lstm_9/lstm_cell/recurrent_kernel', 'bidirectional_5/forward_lstm_9/lstm_cell/bias', 'bidirectional_5/backward_lstm_9/lstm_cell/kernel', 'bidirectional_5/backward_lstm_9/lstm_cell/recurrent_kernel', 'bidirectional_5/backward_lstm_9/lstm_cell/bias', 'lstm_10/lstm_cell/recurrent_kernel', 'lstm_10/lstm_cell/bias', 'lstm_11/lstm_cell/kernel', 'lstm_11/lstm_cell/recurrent_kernel', 'lstm_11/lstm_cell/bias', 'time_distributed_2/dense_2/kernel', 'time_distributed_2/dense_2/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - loss: 0.5961 - mae: 0.1611 - val_loss: 4.1045e-05 - val_mae: 0.1853\n",
      "Epoch 2/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 9.0453e-06 - mae: 0.1528 - val_loss: 1.8163e-10 - val_mae: 0.1852\n",
      "Epoch 3/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.4145e-10 - mae: 0.1515 - val_loss: 1.0831e-10 - val_mae: 0.1852\n",
      "Epoch 4/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 6.6544e-10 - mae: 0.1535 - val_loss: 1.0811e-09 - val_mae: 0.1852\n",
      "Epoch 5/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 6.8887e-10 - mae: 0.1526 - val_loss: 1.0424e-09 - val_mae: 0.1852\n",
      "Epoch 6/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.1667e-09 - mae: 0.1529 - val_loss: 7.1785e-10 - val_mae: 0.1852\n",
      "Epoch 7/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.1075e-09 - mae: 0.1520 - val_loss: 1.0433e-09 - val_mae: 0.1852\n",
      "Epoch 8/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 9.5065e-10 - mae: 0.1526 - val_loss: 1.6559e-09 - val_mae: 0.1852\n",
      "Epoch 9/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.4207e-09 - mae: 0.1524 - val_loss: 1.4221e-09 - val_mae: 0.1852\n",
      "Epoch 10/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.1008e-09 - mae: 0.1520 - val_loss: 1.7584e-09 - val_mae: 0.1852\n",
      "Epoch 11/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.3831e-09 - mae: 0.1518 - val_loss: 1.3898e-09 - val_mae: 0.1852\n",
      "Epoch 12/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 16ms/step - loss: 1.2670e-09 - mae: 0.1535 - val_loss: 1.3099e-09 - val_mae: 0.1852\n",
      "Epoch 13/100\n",
      "\u001b[1m311/311\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 17ms/step - loss: 1.5000e-09 - mae: 0.1529 - val_loss: 1.6951e-09 - val_mae: 0.1852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 959ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 18 and the array at index 1 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 142\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# [10] Forecast Next 24 Hrs, Month, and Year\u001b[39;00m\n\u001b[1;32m    140\u001b[0m last_24_hours \u001b[38;5;241m=\u001b[39m X_scaled[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m24\u001b[39m:]\n\u001b[0;32m--> 142\u001b[0m next_24_hrs \u001b[38;5;241m=\u001b[39m \u001b[43mforecast_traffic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_24_hours\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m next_week \u001b[38;5;241m=\u001b[39m forecast_traffic(model, last_24_hours, \u001b[38;5;241m7\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m24\u001b[39m, scaler_X, scaler_y)\n\u001b[1;32m    144\u001b[0m next_month \u001b[38;5;241m=\u001b[39m forecast_traffic(model, last_24_hours, \u001b[38;5;241m30\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m24\u001b[39m, scaler_X, scaler_y)\n",
      "Cell \u001b[0;32mIn[3], line 133\u001b[0m, in \u001b[0;36mforecast_traffic\u001b[0;34m(model, initial_input, forecast_steps, scaler_X, scaler_y)\u001b[0m\n\u001b[1;32m    130\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(next_pred)\n\u001b[1;32m    132\u001b[0m     next_pred_scaled \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39mtransform(np\u001b[38;5;241m.\u001b[39marray(next_pred)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m--> 133\u001b[0m     new_input \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_pred_scaled\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m new_input\n\u001b[1;32m    136\u001b[0m predictions_real \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(np\u001b[38;5;241m.\u001b[39marray(predictions)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/null/work/Traffic-Forecasting/traffic_forecasting/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:5821\u001b[0m, in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5819\u001b[0m     values \u001b[38;5;241m=\u001b[39m ravel(values)\n\u001b[1;32m   5820\u001b[0m     axis \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 5821\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 18 and the array at index 1 has size 1"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, Dense, TimeDistributed, RepeatVector, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# [1] Define Helper Functions\n",
    "def create_sequences(data, targets, window_size, forecast_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - forecast_horizon):\n",
    "        X.append(data[i : i + window_size])\n",
    "        y.append(targets[i + window_size : i + window_size + forecast_horizon])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# [2] Load and Preprocess Data\n",
    "files = ['./dataset/MA-A.xlsx', './dataset/NORTHBOUND.xlsx', './dataset/SOUTHBOUND.xlsx']\n",
    "sheets = [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "df_list = []\n",
    "for file in files:\n",
    "    if os.path.exists(file):\n",
    "        for sheet in sheets:\n",
    "            temp_df = pd.read_excel(file, sheet_name=sheet)\n",
    "\n",
    "            if 'TRAFFIC STATUS' in temp_df.columns:\n",
    "                temp_df.drop(columns=['TRAFFIC STATUS'], inplace=True)\n",
    "\n",
    "            temp_df['TIME(24 HOUR)'] = temp_df['TIME(24 HOUR)'].astype(str).str.zfill(5) + ':00'\n",
    "            temp_df['Datetime'] = pd.to_datetime(temp_df['DATE'].astype(str) + ' ' + temp_df['TIME(24 HOUR)'], dayfirst=True, errors='coerce')\n",
    "\n",
    "            temp_df.dropna(subset=['Datetime'], inplace=True)\n",
    "            temp_df.sort_values('Datetime', inplace=True)\n",
    "\n",
    "            temp_df['Hour'] = temp_df['Datetime'].dt.hour\n",
    "            temp_df['DayOfWeek'] = temp_df['Datetime'].dt.dayofweek\n",
    "            temp_df['Month'] = temp_df['Datetime'].dt.month\n",
    "\n",
    "            df_list.append(temp_df)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# [3] Encode Categorical Features\n",
    "categorical_cols = ['DAY OF THE WEEK', 'WEATHER', 'ROAD CONDITION', 'HOLIDAY']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Define features and target\n",
    "target_column = 'TRAFFIC VOLUME'\n",
    "features = df.drop([target_column, 'DATE', 'TIME(24 HOUR)', 'Datetime'], axis=1)\n",
    "target = df[target_column]\n",
    "\n",
    "# [4] Normalize Data\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(features)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(target.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# [5] Create Sequences\n",
    "window_size = 24  # Use past 24 hours\n",
    "forecast_horizon = 24  # Fixed forecast horizon for training\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, window_size, forecast_horizon)\n",
    "\n",
    "# Train-Test Split\n",
    "train_size = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:train_size], X_seq[train_size:]\n",
    "y_train, y_test = y_seq[:train_size], y_seq[train_size:]\n",
    "\n",
    "# [6] Build LSTM Seq2Seq Model\n",
    "encoder_inputs = Input(shape=(window_size, X_train.shape[2]))\n",
    "\n",
    "encoder_lstm1 = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01)))(encoder_inputs)\n",
    "encoder_lstm2 = Bidirectional(LSTM(32, return_sequences=False, return_state=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm2(encoder_lstm1)\n",
    "\n",
    "state_h = Concatenate()([forward_h, backward_h])\n",
    "state_c = Concatenate()([forward_c, backward_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = RepeatVector(forecast_horizon)(state_h)\n",
    "\n",
    "decoder_lstm1 = LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2, kernel_regularizer=l2(0.01))\n",
    "decoder_lstm2 = LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)\n",
    "\n",
    "decoder_outputs = decoder_lstm1(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_outputs = decoder_lstm2(decoder_outputs)\n",
    "\n",
    "output_layer = TimeDistributed(Dense(1, activation=\"linear\"))(decoder_outputs)\n",
    "\n",
    "model = Model(inputs=encoder_inputs, outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), metrics=[\"mae\"])\n",
    "\n",
    "# [7] Train Model\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=128,  # Increase batch size\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# [8] Save Model & Scalers\n",
    "model.save(\"traffic_forecasting_seq2seq.h5\")\n",
    "joblib.dump(scaler_X, \"scaler_X.pkl\")\n",
    "joblib.dump(scaler_y, \"scaler_y.pkl\")\n",
    "\n",
    "# [9] Forecasting Function (Supports 24 Hrs, Weeks, Months, Years)\n",
    "def forecast_traffic(model, initial_input, forecast_steps, scaler_X, scaler_y):\n",
    "    predictions = []\n",
    "    input_seq = np.array(initial_input)\n",
    "\n",
    "    for _ in range(forecast_steps):\n",
    "        pred = model.predict(input_seq.reshape(1, input_seq.shape[0], input_seq.shape[1]))\n",
    "        next_pred = pred[0][-1]\n",
    "        predictions.append(next_pred)\n",
    "\n",
    "        next_pred_scaled = scaler_y.transform(np.array(next_pred).reshape(-1, 1)).flatten()\n",
    "        new_input = np.append(input_seq[1:], [next_pred_scaled], axis=0)\n",
    "        input_seq = new_input\n",
    "\n",
    "    predictions_real = scaler_y.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()\n",
    "    return predictions_real\n",
    "\n",
    "# [10] Forecast Next 24 Hrs, Month, and Year\n",
    "last_24_hours = X_scaled[-24:]\n",
    "\n",
    "next_24_hrs = forecast_traffic(model, last_24_hours, 24, scaler_X, scaler_y)\n",
    "next_week = forecast_traffic(model, last_24_hours, 7 * 24, scaler_X, scaler_y)\n",
    "next_month = forecast_traffic(model, last_24_hours, 30 * 24, scaler_X, scaler_y)\n",
    "\n",
    "\n",
    "print(\"Next 24 Hours Forecast:\", next_24_hrs)\n",
    "print(\"Next 1 Week Forecast:\", next_week)\n",
    "print(\"Next 1 Month Forecast:\", next_month)\n",
    "\n",
    "# [11] Plot Training Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e1438-5c18-4097-a1e7-f30076764d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [9] Training Predictions\n",
    "y_train_pred_scaled = model.predict(X_train)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_train_pred = scaler_y.inverse_transform(y_train_pred_scaled.reshape(-1, forecast_horizon))\n",
    "y_train_actual = scaler_y.inverse_transform(y_train.reshape(-1, forecast_horizon))\n",
    "\n",
    "# Select only the first predicted step for each sample\n",
    "y_train_actual_first = y_train_actual[:, 0]  # First actual timestep\n",
    "y_train_pred_first = y_train_pred[:, 0]      # First predicted timestep\n",
    "\n",
    "# Plot actual vs predicted (Training Set)\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_train_actual_first[:250], label=\"Actual\")\n",
    "plt.plot(y_train_pred_first[:250], label=\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted Traffic Volume (Training Set - First 250 Samples)\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Traffic Volume\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f44506f-2904-4c5a-9176-3420422aedb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Model and scalers loaded successfully.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start date for prediction (YYYY-MM-DD):  2025-03-18\n",
      "Enter the end date for prediction (YYYY-MM-DD):  2025-03-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzier/null/work/Traffic-Forecasting/traffic_forecasting/lib/python3.12/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "🔍 Prediction 1/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "🔍 Prediction 2/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 3/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🔍 Prediction 4/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 5/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "🔍 Prediction 6/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 7/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "🔍 Prediction 8/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "🔍 Prediction 9/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 10/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 11/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "🔍 Prediction 12/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "🔍 Prediction 13/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🔍 Prediction 14/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 15/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "🔍 Prediction 16/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "🔍 Prediction 17/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "🔍 Prediction 18/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "🔍 Prediction 19/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
      "🔍 Prediction 20/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 21/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "🔍 Prediction 22/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🔍 Prediction 23/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "🔍 Prediction 24/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "🔍 Prediction 25/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🔍 Prediction 26/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 27/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "🔍 Prediction 28/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "🔍 Prediction 29/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 30/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 31/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 32/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "🔍 Prediction 33/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "🔍 Prediction 34/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "🔍 Prediction 35/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "🔍 Prediction 36/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "🔍 Prediction 37/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 38/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🔍 Prediction 39/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "🔍 Prediction 40/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "🔍 Prediction 41/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "🔍 Prediction 42/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "🔍 Prediction 43/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "🔍 Prediction 44/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 45/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 46/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "🔍 Prediction 47/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "🔍 Prediction 48/48, y_pred_scaled shape: (1, 24, 1)\n",
      "\n",
      "📊 Predicted Traffic Volume:\n",
      "               Datetime  Predicted Traffic Volume\n",
      "0   2025-03-18 00:00:00                194.546249\n",
      "1   2025-03-18 01:00:00                163.939270\n",
      "2   2025-03-18 02:00:00                282.429077\n",
      "3   2025-03-18 03:00:00                924.686157\n",
      "4   2025-03-18 04:00:00               3371.418701\n",
      "5   2025-03-18 05:00:00               4231.707031\n",
      "6   2025-03-18 06:00:00               4050.987305\n",
      "7   2025-03-18 07:00:00               3705.350830\n",
      "8   2025-03-18 08:00:00               3460.862549\n",
      "9   2025-03-18 09:00:00               3234.391846\n",
      "10  2025-03-18 10:00:00               3141.777588\n",
      "11  2025-03-18 11:00:00               3050.764648\n",
      "12  2025-03-18 12:00:00               2852.115967\n",
      "13  2025-03-18 13:00:00               2779.385742\n",
      "14  2025-03-18 14:00:00               3092.460938\n",
      "15  2025-03-18 15:00:00               3271.505859\n",
      "16  2025-03-18 16:00:00               2795.153076\n",
      "17  2025-03-18 17:00:00               1970.851807\n",
      "18  2025-03-18 18:00:00               1449.083252\n",
      "19  2025-03-18 19:00:00               1363.388428\n",
      "20  2025-03-18 20:00:00               1075.872314\n",
      "21  2025-03-18 21:00:00                929.860046\n",
      "22  2025-03-18 22:00:00                839.407471\n",
      "23  2025-03-18 23:00:00                914.350037\n",
      "24  2025-03-19 00:00:00               1054.915405\n",
      "25  2025-03-19 01:00:00               1080.190186\n",
      "26  2025-03-19 02:00:00               1099.428711\n",
      "27  2025-03-19 03:00:00               1106.706787\n",
      "28  2025-03-19 04:00:00               1104.247314\n",
      "29  2025-03-19 05:00:00               1103.061157\n",
      "30  2025-03-19 06:00:00               1103.116577\n",
      "31  2025-03-19 07:00:00               1102.927002\n",
      "32  2025-03-19 08:00:00               1101.790649\n",
      "33  2025-03-19 09:00:00               1098.997559\n",
      "34  2025-03-19 10:00:00               1094.104004\n",
      "35  2025-03-19 11:00:00               1087.750244\n",
      "36  2025-03-19 12:00:00               1079.792480\n",
      "37  2025-03-19 13:00:00               1069.447266\n",
      "38  2025-03-19 14:00:00               1058.268311\n",
      "39  2025-03-19 15:00:00               1050.460083\n",
      "40  2025-03-19 16:00:00               1047.956299\n",
      "41  2025-03-19 17:00:00               1048.363770\n",
      "42  2025-03-19 18:00:00               1048.825073\n",
      "43  2025-03-19 19:00:00               1049.359497\n",
      "44  2025-03-19 20:00:00               1051.060669\n",
      "45  2025-03-19 21:00:00               1052.705444\n",
      "46  2025-03-19 22:00:00               1053.821655\n",
      "47  2025-03-19 23:00:00               1054.009033\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# ✅ Load Model and Scalers\n",
    "try:\n",
    "    model = load_model(\"traffic_forecasting_seq2seq.h5\", custom_objects={\"mse\": MeanSquaredError()})\n",
    "    scaler_X = joblib.load(\"scaler_X.pkl\")\n",
    "    scaler_y = joblib.load(\"scaler_y.pkl\")\n",
    "    print(\"📂 Model and scalers loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model or scalers: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Get User Input\n",
    "start_date = input(\"Enter the start date for prediction (YYYY-MM-DD): \").strip()\n",
    "end_date = input(\"Enter the end date for prediction (YYYY-MM-DD): \").strip()\n",
    "\n",
    "try:\n",
    "    start_datetime = pd.to_datetime(start_date, format='%Y-%m-%d')\n",
    "    end_datetime = pd.to_datetime(end_date, format='%Y-%m-%d')\n",
    "except Exception:\n",
    "    print(\"⚠️ Invalid date format! Use YYYY-MM-DD.\")\n",
    "    exit()\n",
    "\n",
    "if start_datetime == end_datetime:\n",
    "    end_datetime += pd.Timedelta(days=1)\n",
    "\n",
    "forecast_horizon = int((end_datetime - start_datetime).total_seconds() // 3600)\n",
    "if forecast_horizon < 1:\n",
    "    print(\"⚠️ The selected date range must include at least one hour for prediction.\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Load Dataset\n",
    "dataset_path = \"./dataset/NORTHBOUND.xlsx\"\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"⚠️ Dataset file '{dataset_path}' not found!\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df = pd.read_excel(dataset_path)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error loading dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Fix Timestamp Parsing\n",
    "try:\n",
    "    df[\"DATE\"] = pd.to_datetime(df[\"DATE\"].astype(str).str.strip(), errors=\"coerce\")\n",
    "\n",
    "    # ✅ Fix \"TIME(24 HOUR)\" Formatting\n",
    "    df[\"TIME(24 HOUR)\"] = df[\"TIME(24 HOUR)\"].astype(str).str.strip()  # Remove spaces\n",
    "    df[\"TIME(24 HOUR)\"] = df[\"TIME(24 HOUR)\"].apply(lambda x: x.zfill(5))  # Ensure HH:MM format\n",
    "    df[\"TIME(24 HOUR)\"] = pd.to_datetime(df[\"TIME(24 HOUR)\"], format=\"%H:%M\", errors=\"coerce\").dt.hour\n",
    "\n",
    "    # ✅ Create \"Datetime\" column\n",
    "    df[\"Datetime\"] = df[\"DATE\"] + pd.to_timedelta(df[\"TIME(24 HOUR)\"], unit='h')\n",
    "    df.sort_values(\"Datetime\", inplace=True)\n",
    "\n",
    "    if df[\"Datetime\"].isnull().any():\n",
    "        raise ValueError(\"⚠️ Some timestamps could not be parsed.\")\n",
    "\n",
    "    # ✅ Get Last Known Timestamp\n",
    "    last_known_timestamp = df[\"Datetime\"].iloc[-1]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error parsing timestamp from dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Add Missing Derived Features (Hour, DayOfWeek, Month)\n",
    "df[\"Hour\"] = df[\"Datetime\"].dt.hour\n",
    "df[\"DayOfWeek\"] = df[\"Datetime\"].dt.dayofweek\n",
    "df[\"Month\"] = df[\"Datetime\"].dt.month\n",
    "\n",
    "# ✅ One-Hot Encode Categorical Features (Same as Training)\n",
    "categorical_cols = ['DAY OF THE WEEK', 'WEATHER', 'ROAD CONDITION', 'HOLIDAY']\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# ✅ Select Only Relevant Columns\n",
    "try:\n",
    "    feature_columns = scaler_X.feature_names_in_\n",
    "    df = df[feature_columns]  # Keep only columns used during training\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Feature mismatch: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Scale Input Data\n",
    "try:\n",
    "    X_scaled = scaler_X.transform(df.values)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error scaling input data: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Ensure X_scaled Exists\n",
    "if X_scaled.shape[0] < 24:\n",
    "    print(\"⚠️ Not enough historical data (require last 24 hours).\")\n",
    "    exit()\n",
    "\n",
    "# ✅ Get Last 24 Hours of Data for Prediction\n",
    "latest_data = X_scaled[-24:].reshape(1, 24, X_scaled.shape[1])  # (1, 24, features)\n",
    "\n",
    "# ✅ Iterative Multi-Step Forecasting\n",
    "y_pred_scaled_list = []\n",
    "timestamps = []\n",
    "# ✅ Adjust Timestamp Handling to Full Hours\n",
    "current_timestamp = start_datetime.replace(minute=0, second=0, microsecond=0)  # Start at the top of the hour\n",
    "\n",
    "for i in range(forecast_horizon):\n",
    "    y_pred_scaled = model.predict(latest_data)  # Output shape: (1, 24, 1)\n",
    "\n",
    "    print(f\"🔍 Prediction {i+1}/{forecast_horizon}, y_pred_scaled shape: {y_pred_scaled.shape}\")\n",
    "\n",
    "    # ✅ Extract the first step (convert from (1, 24, 1) → (1, 1, 1))\n",
    "    y_pred_single = y_pred_scaled[:, 0, :].reshape(1, 1, 1)\n",
    "\n",
    "    # ✅ Store Prediction\n",
    "    y_pred_scaled_list.append(y_pred_single.flatten())\n",
    "    \n",
    "    # ✅ Ensure timestamp is rounded to the nearest hour\n",
    "    timestamps.append(current_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\"))  # Format as \"YYYY-MM-DD HH:00:00\"\n",
    "    \n",
    "    # ✅ Update input sequence (Shift left, append new prediction)\n",
    "    latest_data = np.hstack((latest_data[:, 1:, :], np.zeros((1, 1, latest_data.shape[-1]))))  # Shift left\n",
    "    latest_data[:, -1, -1] = y_pred_single.flatten()  # Insert new prediction in last column\n",
    "\n",
    "    # ✅ Update current timestamp after each iteration\n",
    "    current_timestamp += pd.Timedelta(hours=1)  # Increment by 1 hour\n",
    "\n",
    "# ✅ Convert Predictions Back to Original Scale\n",
    "y_pred_scaled_array = np.array(y_pred_scaled_list).reshape(-1, 1)\n",
    "y_pred_actual = scaler_y.inverse_transform(y_pred_scaled_array).flatten()\n",
    "\n",
    "# ✅ Display Predictions\n",
    "prediction_df = pd.DataFrame({\"Datetime\": timestamps, \"Predicted Traffic Volume\": y_pred_actual})\n",
    "print(\"\\n📊 Predicted Traffic Volume:\")\n",
    "print(prediction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d250d-e509-4ef0-856a-1cf40cebaccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
